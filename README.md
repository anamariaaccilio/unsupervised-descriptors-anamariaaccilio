# Informe del Hackathon ‚Äì Clasificaci√≥n de Im√°genes con Descriptores Cl√°sicos y SVM

---

## √çndice
1. Introducci√≥n  
2. Dataset y Preprocesamiento  
3. Extracci√≥n de Descriptores  
   - Fisher Vectors  
   - Gaussian Mixture Models (GMM)  
   - Reducci√≥n de dimensionalidad (opcional)  
4. Modelos de Clasificaci√≥n  
   - SVM Lineal  
   - SVM con Kernel RBF  
   - Optimizaci√≥n de Hiperpar√°metros  
   - Exploraci√≥n de Kernel œá¬≤  
5. Resultados  
6. Discusi√≥n  
7. Conclusiones  
8. Integrantes  
9. Referencias  

---

## 1. Introducci√≥n
El objetivo del hackathon fue **clasificar im√°genes del dataset STL-10** utilizando √∫nicamente **t√©cnicas cl√°sicas de visi√≥n por computador**, evitando el uso de redes neuronales profundas.  
Nuestro grupo plante√≥ un pipeline basado en:  

- **Descriptores Fisher Vectors (FV)** para representar im√°genes.  
- **Clasificadores supervisados simples (SVM, k-NN, Logistic Regression)** para la evaluaci√≥n.  
- **Validaci√≥n cruzada y b√∫squeda de hiperpar√°metros** para mejorar la precisi√≥n y la robustez de los modelos.  

---

## 2. Dataset y Preprocesamiento
- **Dataset STL-10**:  
  - 10 clases (avi√≥n, p√°jaro, auto, etc.).  
  - 5,000 im√°genes de entrenamiento (etiquetadas).  
  - 8,000 im√°genes de test.  
  - 100,000 im√°genes no etiquetadas para representaci√≥n no supervisada.  

- **Preprocesamiento**:  
  1. Extracci√≥n de descriptores locales en cada imagen.  
  2. Entrenamiento de un **Gaussian Mixture Model (GMM)** sobre los descriptores locales.  
  3. Representaci√≥n de cada imagen como un **Fisher Vector (FV)** derivado del GMM.  
  4. Guardado en formato `.npy`:  
     - `Xtr_fv.npy` ‚Üí Fisher Vectors de entrenamiento.  
     - `Xte_fv.npy` ‚Üí Fisher Vectors de test.  
     - `ytr.npy`, `yte.npy` ‚Üí etiquetas correspondientes.  

Esto evit√≥ recalcular descriptores cada vez, optimizando el tiempo de experimentaci√≥n.  

---

## 3. Extracci√≥n de Descriptores

### Fisher Vectors (FV)
Los **Fisher Vectors** son una representaci√≥n avanzada que codifica c√≥mo los descriptores locales de una imagen se desv√≠an de un modelo generativo global (en este caso, un GMM).  

Formalmente, dado un GMM con par√°metros \(\lambda = \{w_k, \mu_k, \Sigma_k\}_{k=1}^K\), el FV de una imagen es un vector que combina:  
- Derivadas respecto a las medias \(\mu_k\).  
- Derivadas respecto a las covarianzas \(\Sigma_k\).  

Esto captura informaci√≥n **de primer y segundo orden** sobre la distribuci√≥n de los descriptores locales.  

---

### Gaussian Mixture Model (GMM)
- Entrenamos un **GMM** con \(K\) gaussianas sobre un conjunto de descriptores locales extra√≠dos de im√°genes no etiquetadas.  
- Cada gaussiana modela un ‚Äúcluster‚Äù de patrones visuales.  
- Los FV se construyen al medir cu√°nto contribuye cada imagen a cada gaussiana.  

> Ejemplo: un FV de dimensi√≥n 4096 puede representar la desviaci√≥n de una imagen respecto a 64 gaussianas.  

---

### Reducci√≥n de dimensionalidad
Dado que los FV son de muy alta dimensi√≥n:  
- Se aplic√≥ **PCA (Principal Component Analysis)** en algunos experimentos para reducir la dimensi√≥n y acelerar los clasificadores.  
- Limitaci√≥n: no m√°s de 4096 dimensiones finales (regla del hackathon).  

---

## 4. Modelos de Clasificaci√≥n

### 4.1 SVM Lineal
- Modelo: `LinearSVC(loss="hinge")`.  
- B√∫squeda sobre el hiperpar√°metro `C`.  
- Resultado √≥ptimo:  
  - `C=1`  
  - Accuracy ‚âà **0.6072**  
  - F1_macro ‚âà **0.605**  

Esto sirvi√≥ como **baseline** inicial.  

---

### 4.2 SVM con Kernel RBF
- Modelo: `SVC(kernel="rbf")`.  
- Primer experimento con par√°metros por defecto (`C=10`, `gamma="scale"`) dio:  
  - Accuracy ‚âà **0.6230**  
  - F1_macro ‚âà **0.6221**  

Esto ya super√≥ el baseline lineal.  

üìñ Esto coincide con la literatura: los FV funcionan mejor con kernels no lineales porque preservan la estructura geom√©trica de los descriptores (Perronnin et al., ECCV 2010).  

---

### 4.3 Optimizaci√≥n de Hiperpar√°metros
Se implement√≥ un **GridSearch** manual sobre los hiperpar√°metros:  

- `C ‚àà {0.1, 1, 3, 10}`  
- `gamma ‚àà {"scale", 1e-3, 3e-3, 1e-2}`  

Validaci√≥n cruzada estratificada con 3 folds.  

- **Mejor configuraci√≥n**: `C=3, gamma="scale"`  
- CV-acc ‚âà **0.6016**  
- Accuracy en test ‚âà **0.62+**  

---

### 4.4 Exploraci√≥n de Kernel œá¬≤
- Se intent√≥ `AdditiveChi2Sampler + LinearSVC`.  
- Sin embargo, los FV contienen **valores negativos**, mientras que el kernel œá¬≤ requiere histogramas no negativos.  
- Resultado: el modelo fall√≥ a menos que se aplicaran transformaciones (shift/abs).  
- Conclusi√≥n: el kernel œá¬≤ es m√°s adecuado para descriptores histogramales (ej. **BoVW**, **LBP**).  

---

## 5. Resultados

| Modelo                    | Accuracy | F1-macro |
|----------------------------|----------|----------|
| **SVM Lineal (C=1)**       | 0.6072   | ~0.605   |
| **SVM RBF (C=10)**         | 0.6230   | 0.6221   |
| **SVM RBF (C=3, opt)**     | ~0.62+   | ~0.62+   |

---

## 6. Discusi√≥n
- Los Fisher Vectors demostraron ser **efectivos y consistentes** para la representaci√≥n de im√°genes.  
- El **SVM RBF** fue superior al SVM lineal, validando lo esperado en literatura.  
- La optimizaci√≥n de hiperpar√°metros mediante GridSearch permiti√≥ alcanzar configuraciones m√°s estables.  
- La exploraci√≥n con œá¬≤ fue acad√©micamente interesante pero no aplicable directamente sobre FV.  

---

## 7. Conclusiones
1. Los **Fisher Vectors** son adecuados para el STL-10, capturan informaci√≥n robusta de las im√°genes.  
2. El **SVM RBF** optimizado super√≥ el baseline lineal en Accuracy y F1.  
3. La b√∫squeda de hiperpar√°metros es crucial en kernels no lineales.  
4. No todos los descriptores son compatibles con todos los kernels (ejemplo: œá¬≤ no se ajusta a FV).  
5. Nuestro pipeline logr√≥ **mejorar progresivamente** desde un baseline sencillo hasta un modelo m√°s robusto y competitivo.  

---


## Integrantes
- Ana Accilio
- Sebastian Loza
- Diana √ëa√±es
- Ximena Lindo
- Luis Golac

  
----

## 9. Referencias
- Perronnin, F., S√°nchez, J., & Mensink, T. (2010). *Improving the Fisher Kernel for Large-Scale Image Classification*. ECCV.  
- Zhang, J., Marszalek, M., Lazebnik, S., & Schmid, C. (2007). *Local Features and Kernels for Classification of Texture and Object Categories*. IJCV.  
- Coates, A., Ng, A., & Lee, H. (2011). *An Analysis of Single-Layer Networks in Unsupervised Feature Learning*. AISTATS.  

